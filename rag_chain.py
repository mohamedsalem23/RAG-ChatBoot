import os
import glob
from dotenv import load_dotenv
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough


load_dotenv()
 
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
LANGCHAIN_API_KEY = os.getenv("LANGCHAIN_API_KEY")
LANGCHAIN_TRACING_V2 = os.getenv("LANGCHAIN_TRACING_V2")

# Check keys
missing = []
if not GOOGLE_API_KEY:
    missing.append("GOOGLE_API_KEY")
if not LANGCHAIN_API_KEY:
    missing.append("LANGCHAIN_API_KEY")
if not LANGCHAIN_TRACING_V2:
    missing.append("LANGCHAIN_TRACING_V2")

if missing:
    print(f"âš ï¸ Warning: Missing keys in .env â†’ {', '.join(missing)}")
    print("â¡ï¸ Please add them to your .env file before running the app.")
else:
    print("âœ… All API Keys loaded successfully!")

# ================================
# Example function (replace with your actual logic)
# ================================
def build_rag_chain():
    print("ğŸš€ Building RAG Chain with:")
    print(f"- GOOGLE_API_KEY: {GOOGLE_API_KEY[:6]}... (hidden)")
    print(f"- LANGCHAIN_API_KEY: {LANGCHAIN_API_KEY[:8]}... (hidden)")
    print(f"- LANGCHAIN_TRACING_V2: {LANGCHAIN_TRACING_V2}")
@st.cache_resource
def build_rag_chain(pdf_folder="information"):
    # Load PDFs
    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
    if not pdf_files:
        st.error("Ù…ÙÙŠØ´ Ù…Ù„ÙØ§Øª PDF ÙÙŠ Ø§Ù„ÙÙˆÙ„Ø¯Ø±!")
        return None
    
    all_splits = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)
    
    for pdf_file in pdf_files:
        try:
            loader = PyPDFLoader(pdf_file)
            docs = loader.load()
            splits = text_splitter.split_documents(docs)
            for split in splits:
                split.metadata["source_file"] = os.path.basename(pdf_file)
            all_splits.extend(splits)
        except Exception as e:
            st.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {os.path.basename(pdf_file)}: {str(e)}")
    
    if not all_splits:
        st.error("Ù…ÙÙŠØ´ Ù…Ø­ØªÙˆÙ‰ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡!")
        return None
    
    # Vector Store
    vectorstore = Chroma.from_documents(
        documents=all_splits,
        embedding=HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # Ø¨Ø±Ù…Ø¨Øª Ù…Ø­Ø³Ù†: ÙŠØ·Ù„Ø¨ Ø§Ù„Ø±Ø¯ Ø¨Ù€Markdown Ø¬Ø¯ÙˆÙ„ Ù„Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø¬Ø¯ÙˆÙ„ Ø¯Ø±Ø§Ø³ÙŠ Ø£Ùˆ Ø´ÙŠØ¡ ÙŠØ­ØªØ§Ø¬ ØªÙ†Ø¸ÙŠÙ…
    prompt_template = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate.from_template(
                "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© (HR) ÙÙŠ ÙƒÙ„ÙŠØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø¬Ø§Ù…Ø¹Ø© ÙƒÙØ± Ø§Ù„Ø´ÙŠØ®. "
                "Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø³ÙŠØ§Ø³Ø§ØªØŒ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§ØªØŒ Ø£Ùˆ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø£Ø¯Ù†Ø§Ù‡ ÙÙ‚Ø·. "
                "Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø¬Ø¯ÙˆÙ„ Ø¯Ø±Ø§Ø³ÙŠ Ø£Ùˆ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†Ø¸Ù…Ø© (Ù…Ø«Ù„ Ø£ÙˆÙ‚Ø§ØªØŒ Ù…ÙˆØ§Ø¯ØŒ Ø£Ù…Ø§ÙƒÙ†)ØŒ "
                "Ø£Ø¬Ø¨ Ø¨ØªÙ†Ø³ÙŠÙ‚ Markdown Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ø¶Ø­ (Ø§Ø³ØªØ®Ø¯Ù… | Ù„Ù„Ø®Ù„Ø§ÙŠØ§ØŒ --- Ù„Ù„Ø®Ø· Ø§Ù„ÙØ§ØµÙ„ØŒ ÙˆØ§Ø¬Ø¹Ù„ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ø¶Ø­Ø©). "
                "Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø¬Ø¯ÙˆÙ„Ø§Ù‹ØŒ Ø£Ø¬Ø¨ Ø¨Ù†Øµ Ø¹Ø§Ø¯ÙŠ. "
                "Ø§Ù„Ø³ÙŠØ§Ù‚ Ù‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬Ø¯Ø§ÙˆÙ„ Ø¯Ø±Ø§Ø³ÙŠØ© ØºÙŠØ± Ù…Ù†Ø¸Ù…Ø© Ø£Ùˆ Ù†ØµÙˆØµ ÙÙˆØ¶ÙˆÙŠØ©ØŒ Ù„Ø°Ø§ Ù‚Ù… Ø¨ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ§Ø¶Ø­Ø©. "
                "Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ØºÙŠØ± ÙƒØ§ÙÙØŒ Ù‚ÙˆÙ„ 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©' Ù…Ø¹ Ø°ÙƒØ± Ø§Ù„Ù…ØµØ¯Ø±. "
                "Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø£Ùˆ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±ØŒ "
            ),
            HumanMessagePromptTemplate.from_template("""
            Ø§Ù„Ø³ÙŠØ§Ù‚:
            {context}

            Ø§Ù„Ø³Ø¤Ø§Ù„:
            {question}
            """)
        ]
    )
    
    # LLM
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
    parser = StrOutputParser()
    
    # RAG Chain
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt_template
        | llm
        | parser
    )
    
    return rag_chain
